# Serving an NLP Model (Transformer) using FastAPI

This repository provides a step-by-step guide for serving a Question Answering (QA) model based on the DistillBERT transformer architecture using FastAPI.
The model is pre-trained on the SQUAD dataset (Stanford Question Answering Dataset).



## Part 1:  Serve an NLP model (transformer) using FastAPI

![FastAPI](Lab - Model testing/Screens/FastAPI_QA.png)
![FastAPI](Lab - Model testing/Screens/FastAPI_QA_Response.png)
![FastAPI](Lab - Model testing/Screens/FastAPI_QA_postman.png)

## Part 2: Containerizing the API using Docker

![FastAPI](Lab - Model testing/Screens/FastAPI_Docker.png)

## Part 3: Serving a Transformer Model using TFX



## Part 4: Load Testing using Locust

![FastAPI](Lab - Model testing/Screens/locust_fastapi_QA.png)
![FastAPI](Lab - Model testing/Screens/results_locust.png)

